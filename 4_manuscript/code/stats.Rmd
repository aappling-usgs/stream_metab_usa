---
title: "Statistics for Manuscript"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---
 
```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(readr)
```

Load data from cached data release files.
```{r, message=FALSE}
dailies_zip='../../4_data_release/cache/models/post/daily_predictions.zip'
diagnostics_rds='../../4_data_release/cache/models/diagnostics.rds'

stats_tmp_dir <- file.path(tempdir(), 'stats')
unzipped <- list(
  dailies = unzip(zipfile=dailies_zip, exdir=stats_tmp_dir)
)
dailies <- readr::read_tsv(unzipped$dailies)
diagnostics <- readRDS(diagnostics_rds)
```

Dates per site
```{r}
dates_per_site <- dailies %>% group_by(site_name) %>% count()
range(dates_per_site$n) # 61 3296
max(dates_per_site$n)/365.25 # 9.02
median(dates_per_site$n)/365.25 # 3.229295
mean(dates_per_site$n)/365.25 # 3.775365
```

Total number of dates
```{r}
nrow(dailies) # 490907
```

Density of dates
```{r}
date_densities <- dailies %>% group_by(site_name) %>% 
  summarize(
    dates_per_site = n(),
    years_per_site = dates_per_site/365.25,
    date_range = as.numeric(diff(range(date)), units='days') + 1,
    year_range = date_range / 365.25,
    date_density = dates_per_site / year_range
  )
date_densities$dates_per_site %>% summary # 61 to 3296 dates
date_densities$years_per_site %>% summary # 0.17 to 9.02 years
date_densities$date_range %>% summary # 62 to 3379 days
date_densities$year_range %>% summary # 0.1698 to 9.25 years
date_densities$date_density %>% summary # 17.3 to 365.25 dates per year
filter(date_densities, date_density < 90)
```

Dates by resolution
```{r}
dates_by_resolution <- dailies %>% group_by(resolution) %>% count()
dates_by_resolution
```

Higher temporal resolution leads to longer runtime
```{r}
ggplot(diagnostics, aes(x=n_dates, y=run_hrs, color=resolution)) + geom_point() + geom_smooth(method='lm') + facet_wrap(~ saved_steps)
```

Higher resolution appears to reduce probability of non-convergence, as measured by whether we re-ran the model
```{r}
diagnostics %>%
  mutate(res = as.integer(gsub('min','',resolution))) %>%
  group_by(res) %>%
  summarize(
    ss_500 = length(which(saved_steps == 500)),
    ss_2000 = length(which(saved_steps == 2000)),
    frac_rerun = ss_2000/(ss_500+ss_2000))
# res ss_500 ss_2000 frac_rerun
#   5     17       1     0.0556
#  12      1       1     0.500 
#  15    213      83     0.280 
#  30     34      24     0.414 
#  60     23      36     0.610 
```

Some measures of total, average, and median runtime
```{r}
diagnostics %>% 
  mutate(hrs_per_year_bymodel=365.25*run_hrs/n_dates) %>%
  summarize(
    run_hrs=sum(run_hrs),
    run_days=run_hrs/24,
    n_dates=sum(n_dates),
    hrs_per_year_q05=quantile(hrs_per_year_bymodel, probs=0.05),
    hrs_per_year_q50=quantile(hrs_per_year_bymodel, probs=0.50),
    hrs_per_year_q95=quantile(hrs_per_year_bymodel, probs=0.95),
    hrs_per_year=365.25*run_hrs/n_dates
  )
```

Determine which data sources were used for each timeseries as inputs to the models
```{r}
config <- read_tsv('../../4_data_release/cache/models/config.tsv')
src_cols <- grep('.src', names(config), fixed=TRUE, value=TRUE)
src_counts <- bind_rows(lapply(setNames(nm=src_cols), function(metab_var_src) {
  counts <- table(config[[metab_var_src]])
  metab__var <- gsub('.src', '', metab_var_src, fixed=TRUE)
  data_frame(variable=metab__var, var=mda.streams::get_var_src_codes(metab_var==metab__var)$var[1],
             src=names(counts), var_src=paste(var, src, sep='_'), count=unname(counts))
}))
src_counts
```
How many sites that were modeled had catchments, and how many sites with catchments were modeled?
```r
# numbers for manuscript
diag <- readr::read_tsv('../4_data_release/cache/models/config.tsv')
catchments <- remake_smu('catchments_as_written', '4_release_spatial.yml')
catchdat <- catchments@data %>%
  full_join(select(diag, site, tag), by=c(site_nm='site'))
(no_model <- catchdat %>% filter(is.na(tag)) %>% nrow()) #38
(mod_no_catch <- catchdat %>% filter(is.na(dat_src)) %>% nrow()) #23
```

How many sites (as opposed to models) went with depth_calcDischRaymond?
```{r}
config %>% filter(depth.src == 'calcDischHarvey') %>% pull(site) %>% table() %>% length()
config %>% filter(depth.src == 'calcDischRaymond') %>% pull(site) %>% table() %>% length()
```

Determine how many K ~ Q nodes there were per model.
```{r}
config <- read_tsv('../../4_data_release/cache/models/config.tsv')
range(sapply(config$K600_lnQ_nodes_centers, function(expr) length(eval(parse(text=expr)))))
median(sapply(config$K600_lnQ_nodes_centers, function(expr) length(eval(parse(text=expr)))))
hist(sapply(config$K600_lnQ_nodes_centers, function(expr) length(eval(parse(text=expr)))))
```

Count sites with and without catchments.
```{r}
# only 132 of 365 modeled sites have struct flags ?!
sites <- readr::read_tsv('../../4_data_release/cache/site_data.tsv')
dim(sites)
sites %>% filter(!is.na(struct.canal_flag) & !is.na(struct.dam_flag) & !is.na(struct.npdes_flag)) %>% nrow
sites %>% filter(is.na(struct.canal_flag) | is.na(struct.dam_flag) | is.na(struct.npdes_flag)) %>% nrow

# 333 modeled sites have associated catchments
catchments <- readOGR('../../1_spatial/cache/catchment_shapefile/catchment_shapefile.shp', stringsAsFactors = FALSE)
length(which(catchments@data$site_nm %in% sites$site_name))

# dailies <- readr::read_tsv('../../4_data_release/cache/models/post/daily_predictions.zip') # no way - read_tsv can read a zip?? brilliant!
# msites <- sort(unique(dailies$site_name))
# sites %>% filter(site_name %in% msites) %>% nrow
# sites %>% filter(site_name %in% msites) %>% filter(!is.na(struct.canal_flag)) %>% nrow
# sites %>% filter(site_name %in% msites) %>% filter(!is.na(struct.dam_flag)) %>% nrow
# sites %>% filter(site_name %in% msites) %>% filter(!is.na(struct.npdes_flag)) %>% nrow
```
